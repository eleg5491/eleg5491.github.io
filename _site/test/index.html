<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Notes for ELEG5491 Introduction to Deep Learning</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for CUHK ELEG5491: Introduction to Deep Learning.">
    <link rel="canonical" href="http://localhost:4000/test/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

    <!-- Google tracking -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
</head>


    <body>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="/">Notes for ELEG5491 Introduction to Deep Learning</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <h2 id="initialization-and-normalization-in-deep-neural-networks">Initialization and Normalization in Deep Neural Networks</h2>
<p>Feb 28, 2017. Wei YANG</p>

<p>platero.yang (at) gmail.com</p>

<h3 id="table-of-contents">Table of Contents:</h3>

<ul>
  <li><a href="#intro">Intro</a></li>
  <li><a href="#init">Initialization of the parameters</a>
    <ul>
      <li><a href="#naive">Naive initialization</a></li>
      <li><a href="#xavier">Xavier initialization</a></li>
      <li><a href="#msr">ReLU: MSR initialization</a></li>
    </ul>
  </li>
  <li><a href="#normalization">Normalization</a>
    <ul>
      <li><a href="#inputnorm">Normalizing the inputs</a></li>
      <li><a href="#batchnorm">Batch (Re)normalization: normalizing the input for each layer</a></li>
      <li><a href="#weightnorm">Normalizing the weights</a></li>
      <li><a href="#layernorm">Normalizing in recurrent neural networks</a></li>
    </ul>
  </li>
  <li><a href="#summary">Outro</a></li>
</ul>

<p><a name="intro"></a></p>
<h3 id="introduction">Introduction</h3>
<p>In this note, we will focus on training neural networks efficiently by appropriate weight initialization and by the normalization techniques.</p>

<p><a name="init"></a></p>
<h3 id="initialization-of-the-parameters">Initialization of the parameters</h3>
<p>The initial value of the network parameters can affect the training process significantly. If the weights are initialized very large, then the nonlinear activation function (e.g., Sigmoid and Tanh) might saturate, which makes the gradients very small. If the weights are initialized very small, then the gradients would be small too. SGD methods with small gradients update the weight slowly, which slows down the training process. One might try to initialize all weights to zero. However, this is a <a href="http://cs231n.github.io/neural-networks-2/#init">common pitfall</a> discussed in cs231n:</p>

<blockquote>
  <p>If every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.</p>
</blockquote>

<p><a name="naive"></a></p>
<h4 id="naive-initialization">Naive initialization</h4>
<p>Intuitively, the weights are expected closed to zero. Hence we usually initialize the weights from a Gaussian distribution with zero mean and a small variance, e.g.,</p>

<script type="math/tex; mode=display">W_{ij} \sim N(0, 0.01^2).</script>

<p>Although it is widely used in practice, there is no implicit evidence that why we set the standard variation as 0.01.</p>

<p><a name="xavier"></a></p>
<h4 id="xavier-initialization">Xavier initialization</h4>
<p>Our derivation mainly following [xxx]. The idea is to keep the input and output of a layer with the same variance and the zero mean. A building block of a conventional neural network consists of a linear layer and an elementwise activation function <script type="math/tex">f(\cdot)</script>,</p>

<script type="math/tex; mode=display">\begin{cases} 
  \mathbf{y}^l = \mathbf{w}_l\mathbf{x}^l + \mathbf{b}^l ,\\
  \mathbf{x}^{l+1} = f(\mathbf{y}^l),
\end{cases}</script>

<p>where <script type="math/tex">\mathbf{x}</script> is the input and <script type="math/tex">\mathbf{y}</script> is the output. <script type="math/tex">W_l</script> is the weight and <script type="math/tex">\mathbf{b}</script> is the bias. <script type="math/tex">l</script> indexes the layer.</p>

<p>We drop the <script type="math/tex">l</script> for simplicity. We assume that the elements in <script type="math/tex">W</script> are independent and identically distributed (i.i.d.), and the elements in <script type="math/tex">\mathbf{x}</script> are also i.i.d. And <script type="math/tex">W</script> and  <script type="math/tex">\mathbf{x}</script> are indepent with each other. Suppose we use the sigmoid function as the nonlinear activation function, then <script type="math/tex">\mathbf{x}</script> has zero mean (<script type="math/tex">E[(x_i)]=0</script>). Since <script type="math/tex">y = \sum_{i=1}^n W_{i} x_i</script>, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Var}(y) &= \text{Var}(\sum_i^n w_ix_i) \\\\
&= \sum_i^n \text{Var}(w_ix_i) \\\\
&= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\\\
&= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \\\\
&= \left( n \text{Var}(w) \right) \text{Var}(x).
\end{align} %]]></script>

<p>Here we let <script type="math/tex">E(w_i)</script> be zero. The above equation shows that the variance of the output is the variance of the input, but scaled by <script type="math/tex">nVar(w)</script>. So if we want to keep the variance of the input and output unchanged, it must have  <script type="math/tex">nVar(W) = 1</script>, which results in,</p>

<script type="math/tex; mode=display">Var(w) = \frac{1}{n}.</script>

<p>Hence the weights should be initialized from randomly drawn from a distribution with zero mean and standard variation <script type="math/tex">\frac{1}{\sqrt{n}}</script>.</p>

<p><a name="msr"></a></p>
<h4 id="relu-activation-msr-initialization">ReLU activation: MSR initialization</h4>
<p>When the activation function is <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>: <script type="math/tex">f(x)=\max(0,x)</script>, the mean of the input <script type="math/tex">E(x)</script> is no longer zero. In this case, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Var}(y) &= \text{Var}(\sum_i^n w_ix_i) \\\\
&= \sum_i^n \text{Var}(w_ix_i) \\\\
&= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \\\\
&= \sum_i^n \text{Var}(w_i)\{E[(x_i)]^2 + \text{Var}(x_i)\} \\\\
&= \sum_i^n \text{Var}(w_i)E(x_i^2) = n \text{Var}(w)E(x^2).
\end{align} %]]></script>

<p>where</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
E[x^2] &= \int_{-\infty}^{+\infty} \max(0,y)^2 p(y) dy \\
&= \int_{0}^{+\infty} y^2 p(y) dy \\
&= \frac{1}{2}\int_{-\infty}^{+\infty} y^2 p(y) dy \\
&= \frac{1}{2} E[(y - E[y])^2] = \frac{1}{2} Var[y]
\end{align} %]]></script>

<p>(see <a href="http://stats.stackexchange.com/questions/138035/variance-calculation-relu-function-deep-learning">Variance caculation ReLU function</a>) By replacing <script type="math/tex">E[x^2]</script> by <script type="math/tex">\frac{1}{2} Var[y]</script>, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Var}(y) &= n \text{Var}(w)\cdot \frac{1}{2} Var[y]\\
\Rightarrow \text{Var}(w) &= \frac{2}{n}.
\end{align} %]]></script>

<p>This is the so called MSR initialization: when we use ReLU as the activation function, the weights should be randomly sampled from distribution with zero mean and <script type="math/tex">\frac{2}{n}</script> variance.</p>

<p><a name="Normalization"></a></p>
<h3 id="normalization">Normalization</h3>

<p>Another way to accelerate the training and simplify the optimization process is to use <strong>normalization</strong>. Normalization usually rescales the data or the weights to make the scale is <em>just right</em> for the training process. In this section, we will discuss several normalization methods for accelerating and simplifying the training of deep neural networks.</p>

<p><a name="inputnorm"></a></p>
<h4 id="normalizing-the-inputs">Normalizing the inputs</h4>

<p>A common practice to normalize the input data is to compute <script type="math/tex">x_i \leftarrow   \frac{x_i - \mu}{\sigma},</script>
where <script type="math/tex">\mu</script> and <script type="math/tex">\sigma</script> are the sample mean and standard deviation, respectively. Intuitively, subtracting the mean reduces the <em>shift</em> of the data, and dividing by the standard deviation removes the <em>scale</em> of the data. In statistics, this procesure is called standardizing, and the standardised data is called the z-score.</p>

<p>Another trick is to decorrelate the inputs: if the inputs are uncorrelated, then the weights are independent with each other, which simplifies the problem. One possible way to decorrelated the inputs is to <a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">whitening the data with PCA</a>. But sometimes PCA can be harmful for your problem, please see the <a href="http://blog.explainmydata.com/2012/07/should-you-apply-pca-to-your-data.html">discussion</a> for further understanding.</p>

<p><a name="batchnorm"></a></p>
<h4 id="batch-renormalization-normalizing-the-input-for-each-layer">Batch (Re)normalization: normalizing the input for each layer</h4>
<h5 id="internal-covariate-shift">Internal Covariate Shift</h5>
<p>Training deep neural networks is difficult due to the changing of the distribution of each layerâ€™s inputs after updating the parameters of the network. It slows down the training by requiring relatively small learning rate and careful weight initialization, espetially for networks with saturating nonlinearities. This phenomenon is refered as <em>internal covariate shift</em>.</p>

<h5 id="batch-normalization">Batch Normalization</h5>
<p>The key is to reduce the internal covariate shift. Motivated by standardizing the input data, we raise such a question: can we normalize the input of <em>each layer</em>? This is exactly what <a href="https://arxiv.org/abs/1502.03167">batch normalization (BN)</a> does: it normalize the input of each layer for each training mini-batch. It has two benefits:</p>
<ol>
  <li>BN allows higher learning rates and cares less about the initialization.</li>
  <li>It also acts as a regularizer.</li>
</ol>

<p>For a layer with <script type="math/tex">d</script>-dimensional input <script type="math/tex">\mathbf{x}=(x^{(1)},\cdots, x^{(d)})</script>,  we will normalize each dimention by</p>

<script type="math/tex; mode=display">\hat{x}^{(k)} = \frac{x^{(k)} - E{[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}},</script>

<p>where <script type="math/tex">E{[x^{(k)}]}</script> and <script type="math/tex">\sqrt{Var[x^{(k)}]}</script> are the sample mean and standard deviation in a mini-batch. To ensure that the normalized activation keeps the original representation power, BN introduces a pair of parameters,</p>

<p><script type="math/tex">y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}.</script>
We can see that when <script type="math/tex">\gamma^{(k)} = \sqrt{Var[x^{(k)}]}</script> and <script type="math/tex">\beta^{(k)} = E{[x^{(k)}]}</script> the original activations can be recovered.</p>

<p>It is easy to compute the gradients by chain rule. You may verify your derivation from the <a href="https://arxiv.org/abs/1502.03167">BN paper</a>.</p>

<h5 id="testinginference">Testing/Inference</h5>
<p>During trainig, we compute the mean and variance for each mini-batch. But for testing/inference, we want the output to depend only on the input, deterministically. Recall that, once the training has finished, we can compute the mean and the unbiased variance by using the population. Hence during testing, we use the polulation mean and unbiased variance estimation instead of the mini-batch mean and variance.</p>

<h5 id="batch-renormalization">Batch Renormalization</h5>
<p>It seems BN has solved everything. E.g., it allows large learning rates, and it is not sensitive to initialization. But <em>what if the batch size is quite small</em>? For small minibatches, the estimates of mean and variance are less accurate. The error accumulates as the depth increases. Moreover, the global estimates of mean and variance are also inaccurate, which may affect the inference/testing accuracy on a large margin!</p>

<p>Let <script type="math/tex">\mu_B, \sigma_B</script> be the minibatch statistics, and <script type="math/tex">\mu, \sigma</script> be their moving averages, then the results of these two normalizations are related by an affine transform,</p>

<script type="math/tex; mode=display">\frac{x_i - \mu}{\sigma} = \frac{x_i - \mu_B + \mu_B - \mu}{\sigma_B}\cdot\frac{\sigma_B}{\sigma} = \frac{x_i - \mu_B}{\sigma_B}\cdot\frac{\sigma_B}{\sigma} + \frac{\mu_B-\mu}{\sigma} = \frac{x_i - \mu_B}{\sigma_B}\cdot r + d,</script>

<p>where <script type="math/tex">r=\frac{\sigma_B}{\sigma}</script> and <script type="math/tex">d = \frac{\mu_B-\mu}{\sigma}</script>. Batch normalization simply set <script type="math/tex">r=1, d=0</script>. We refer this augmented batch normalization as Batch Renormalization: <em>the fixed (for the given minibatch) <script type="math/tex">r</script> and <script type="math/tex">d</script> correct for the fact that the minibatch statistics differ from the population ones.</em></p>

<p>In optimization, we treat <script type="math/tex">r, d</script> as constant to compute the gradients using backpropagation. For more details, please refer to <a href="https://arxiv.org/abs/1702.03275">the original paper</a>.</p>

<p>Limitation: for a fixed length feed-forward neural network, BN simply stores the statistics for each layer separately. However, for recurrent neural networks with varied length of sequence as input, applying BN requires different statistics for different time-steps. This limits the application of BN in recurrent neural networks directly. In the following section, we will see how <em>weight normalization</em> and <em>layer normalization</em> solve this problem.</p>

<p><a name="weightnorm"></a></p>
<h4 id="normalizing-the-weights">Normalizing the weights</h4>

<p>Instead of normalizing the input of each layer, weight normalization normalizes the weight of each layer by decouple the weight direction and the weight magnitude,</p>

<script type="math/tex; mode=display">\mathbf{w} = g\frac{\mathbf{v}}{\|\mathbf{v}\|}.</script>

<p>Here <script type="math/tex">g = \|\mathbf{w}\|</script> is a scalar, <script type="math/tex">\mathbf{v}</script> is a vector with the same dimentionality of <script type="math/tex">\mathbf{w}</script>, and <script type="math/tex">\|\mathbf{v}\|</script> is the Euclidean norm of <script type="math/tex">\mathbf{v}</script>. This reparameterization decouples the weight magnitude <script type="math/tex">g</script> from its direction <script type="math/tex">w</script>.</p>

<p><a name="layernorm"></a></p>
<h4 id="normalizing-in-recurrent-neural-networks">Normalizing in recurrent neural networks</h4>

<p>Layer normalization proposes to reduce the covariate shift problem by fixing the mean and the variance of the summed inputs within each layer.</p>

<script type="math/tex; mode=display">\mu = \frac{1}{H} \sum_{i=1}^H y_i, \sigma^2 = \frac{1}{H} \sum_{i=1}^H (y_i - \mu)^2,</script>

<p>where <script type="math/tex">H</script> is the number of activations (neurons), <script type="math/tex">y_i = \mathbf{w}_i^T\mathbf{x}</script>. In this way, all the hidden neurons in a layer share the same statistics $\mu, \sigma$, but different training cases have different normalization terms. While for batch normalization, the statistics depend on mini-batches. Hence layer normaliztion does not impose any constraint on the size of a mini-batch and it can be used in the pure online regime with batch size 1.</p>

<h3 id="outro">Outro</h3>
<p>Initialization and normalization are essential for training neural networks. Very deep neural networks, e.g., GoogLeNet and ResNet, are built upon stacks of these methods. Although they seem tricky, they are developed by intuitive motivations and proper assumptions.</p>

<p>As discussed in layer normalization, all the normalization methods can be summarized in the following transformation</p>

<script type="math/tex; mode=display">y_i = f(\frac{g_i}{\sigma_i}(\mathbf{w}^T\mathbf{x} - \mu_i) + b_i,</script>

<p>In batch normalization, <script type="math/tex">\mu_i, \sigma_i</script> are computed based on minibatches. While in layer normalization, the statistics are computed by the activations themselves. In weight normalization, <script type="math/tex">\mu_i = 0</script> and <script type="math/tex">\sigma_i= \|\mathbf{w}\|_2</script>. With careful observations and assumptions, you may develop better initialization/normalization methods.</p>

<div style="clear:both;">
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//deep-learning.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
</div>


  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        <li>
          <a href="https://github.com/eleg5491">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">eleg5491</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/dl_cuhk">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">dl_cuhk</span>
          </a>
        </li>
        <li>
          <!-- <a href="mailto:"></a> -->
          Design courtesy by <a href="http://cs231n.github.io/">cs231n notes</a>.
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>